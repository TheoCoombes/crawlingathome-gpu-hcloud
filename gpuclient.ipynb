{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone \"https://github.com/TheoCoombes/crawlingathome\" crawlingathome_client\n",
    "! pip3 install -r crawlingathome_client/requirements.txt --no-cache-dir\n",
    "! rm requirements.txt\n",
    "! wget https://raw.githubusercontent.com/rvencu/crawlingathome-gpu-hcloud/staged-clients/requirements.txt\n",
    "! pip3 install -r ./requirements.txt --no-cache-dir\n",
    "! pip3 install ftfy pandas tfr_image\n",
    "! pip3 install tensorflow --no-cache-dir\n",
    "! pip3 install clip-anytorch\n",
    "! yes | pip3 uninstall pillow\n",
    "! CC=\"cc -mavx2\" pip3 install -U --force-reinstall pillow-simd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title GPU controlled Hetznet Cloud swarm of workers\n",
    "YOUR_NICKNAME_FOR_THE_LEADERBOARD = \"Colab-GPU-hcloud\" #@param {type:\"string\"}\n",
    "groupsize = 16 #@param {type:\"string\"}\n",
    "CRAWLINGATHOME_SERVER_URL = \"http://cah.io.community/\"\n",
    "groupsize = int(groupsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'colorama'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-158732134bec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcolorama\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatistics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./crawlingathome-worker/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'colorama'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "import clip\n",
    "import torch\n",
    "import pickle\n",
    "import shutil\n",
    "import threading\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from statistics import mode\n",
    "import crawlingathome_client as cah\n",
    "sys.path.append('./crawlingathome-worker/')\n",
    "from multiprocessing import JoinableQueue, Process, cpu_count\n",
    "\n",
    "if not os.path.exists(\"./stats/\"):\n",
    "    os.makedirs(\"./stats/\")\n",
    "if not os.path.exists(\"./save/\"):\n",
    "    os.makedirs(\"./save/\")\n",
    "\n",
    "# initial cleanup - delete all working files in case of crash recovery\n",
    "reg_compile = re.compile(r\"^\\d{1,3}-\\d{1,3}-\\d{1,3}-\\d{1,3}$\")\n",
    "for root, dirnames, filenames in os.walk(\".\"):\n",
    "    for filename in filenames:\n",
    "        if filename.startswith(\"gpujob.zip_\"):\n",
    "            os.remove(filename)\n",
    "    for dir in dirnames:\n",
    "        if reg_compile.match(dir):\n",
    "            shutil.rmtree(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize joinable queues to transfer messages between multiprocess processes\n",
    "# Outbound queues, we need one for each io worker\n",
    "outbound = []\n",
    "for _ in range(2 * groupsize): # we need 2x IO workers to keep GPU permanently busy\n",
    "        outbound.append(JoinableQueue())\n",
    "inbound = JoinableQueue()\n",
    "counter = JoinableQueue()\n",
    "inpsize = JoinableQueue() # use this to communicate number of jobs downloading now\n",
    "gpuflag = JoinableQueue() # use this to flag that gpu is processing"
   ]
  },
  {
   "source": [
    "# define CLIP class around OpenAI clip model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "class CLIPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, preprocess):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_transform = preprocess\n",
    "        self.tokenizer = clip.tokenize\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.dataframe.iloc[index]\n",
    "        return (\n",
    "            self.image_transform(Image.open(row[\"PATH\"])),\n",
    "            self.tokenizer(row[\"TEXT\"], truncate_text=True)[0],\n",
    "        )\n",
    "\n",
    "class CLIP:\n",
    "    def __init__(self):\n",
    "        self.model, self.preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "        self.cosine_similarity = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        with torch.no_grad():\n",
    "            self.categories = self.model.encode_text(clip.tokenize([\"neutral\",\"selfie\", \"illustration, drawing\", \"toys, play, kids, children\", \"teddy bear, puppet\", \"animal, bird, mammal, insect\" \"fashion, clothes\", \"logo, commercial, ad, advertisement\", \"drawing, painting\",\"anime, cartoon\",\"comedy, fun\",\"romance, love story\",\"thriller, suspense, crime story\",\"action, action movie\", \"horror, monster movie\", \"documentary\", \"news, journalism\", \"entertainment\", \"talk show\", \"porn, sex, sperm, nipples, breats, tits, boops, penis, dick, cock, clitoris, vagina, fuck, lust, horny, sexual, lick, licking\",  \"porn, sex, sperm, nipples\", \"porn, sex, sperm, penis, dick, cock\", \"nipples, breats, tits, boops, sexy\", \"penis, dick, cock\", \"clitoris, vagina\", \"sex, fuck, lust, horny, sexual, lick, licking\", \"porn, sex, sexy\",\"sexy, hot\",\"sperm, skin\",\"lust, horny, sexual\",\"lick, licking, body\", \"anime, hentai, sexy\", \"cartoon, sexy, sex\", \"hentai\", \"anime, sexy, breasts\", \"hentai\"]).to(device))\n",
    "            self.underaged_categories = self.model.encode_text(clip.tokenize([\"teenager, teen\", \"kid, child, teenager, teen, baby or toddler, underaged, little girl, little boy\", \"kid, child, little girl, little boy\", \"baby, toddler\",\"adult, woman, man, grownup, grown person,full-aged of legal age\",\"full-aged, of legal age, adult\",\"woman, man\",\"adult, woman, man, grownup, grown person,full-aged of legal age\"]).to(device))\n",
    "            self.animal_categories = self.model.encode_text(clip.tokenize([\"lifeless object, thing\", \"thing, object\", \"material\", \"furniture\",\"wall\", \"house\", \"tree\", \"wood\",\"ground\",\"industry\", \"table\", \"bed\", \"tool\", \"dress, clothes\", \"door\", \"chair\", \"rock, stone\", \"human\", \"man\", \"woman\", \"man, woman\", \"animal\",\"cat\",\"dog\", \"cow\", \"pig\", \"goat\", \"sheep\", \"elephant\", \"horse\", \"horse, elephant, pig, dog, cat, sheep, goat, animal\", \"life\", \"wildlife\"]).to(device))\n",
    "\n",
    "    def similarity_imgalt(self, image_tensor, text_tokens):\n",
    "        with torch.no_grad():\n",
    "            image_features = self.model.encode_image(image_tensor.to(device)).float()\n",
    "            text_features = self.model.encode_text(text_tokens.to(device)).float()\n",
    "            similarity = self.cosine_similarity(image_features, text_features).tolist()\n",
    "\n",
    "        image_features = image_features.detach().cpu().numpy()\n",
    "        return image_features, similarity\n",
    "\n",
    "    def preprocess_images(self, df):\n",
    "        ret_image_features = []\n",
    "        ret_similarity = []\n",
    "        batch_size = 256 if device == \"cuda\" else 8\n",
    "        dataset = CLIPDataset(df, self.preprocess)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=int(2*cpu_count()/3), pin_memory=True)\n",
    "        for tensors, tokens in dataloader:\n",
    "            image_features, similarities = self.similarity_imgalt(tensors, tokens)\n",
    "            ret_image_features.extend(image_features)\n",
    "            ret_similarity.extend(similarities)\n",
    "        return ret_image_features, ret_similarity\n",
    "\n",
    "    def prob(self, image_features, text_features):\n",
    "        text_features = text_features.float()\n",
    "        image_features = torch.as_tensor(image_features).to(device, dtype=torch.float32)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "        _, indices = similarity.topk(2)\n",
    "        return indices\n",
    "\n",
    "\n",
    "clip_filter = CLIP()\n",
    "\n",
    "\n",
    "def df_clipfilter(df):\n",
    "    sim_threshold = 0.3\n",
    "    underaged_text = [\"teen\", \"kid\", \"child\", \"baby\"]\n",
    "\n",
    "    img_embedding, similarities = clip_filter.preprocess_images(df)\n",
    "    tmp_embed = []\n",
    "\n",
    "    for i, img_embed in enumerate(img_embedding):\n",
    "        if similarities[i] < sim_threshold:\n",
    "            #df.drop(i, inplace=True)\n",
    "            df.at[i, 'dropped'] = True\n",
    "            continue\n",
    "\n",
    "        # get most similar categories\n",
    "        nsfw_prob = clip_filter.prob(img_embed, clip_filter.categories)\n",
    "        df.at[i, \"NSFW\"] = \"UNSURE\"\n",
    "        df.at[i, \"similarity\"] = similarities[i]\n",
    "        if nsfw_prob[0] < 19 and nsfw_prob[1] < 19:\n",
    "            df.at[i, \"NSFW\"] = \"UNLIKELY\"\n",
    "            tmp_embed.append(img_embed)\n",
    "            continue\n",
    "        elif nsfw_prob[0] >= 19 and nsfw_prob[1] >= 19:\n",
    "            df.at[i, \"NSFW\"] = \"NSFW\"\n",
    "\n",
    "        underage_prob = clip_filter.prob(img_embed, clip_filter.underaged_categories)\n",
    "        if underage_prob[0] < 4 or underage_prob[1] < 4 or any(x in df.at[i, \"TEXT\"] for x in underaged_text):\n",
    "            #df.drop(i, inplace=True)\n",
    "            df.at[i, 'dropped'] = True\n",
    "            continue\n",
    "\n",
    "        animal_prob = clip_filter.prob(img_embed, clip_filter.animal_categories)\n",
    "        if animal_prob[0] > 20:\n",
    "            #df.drop(i, inplace=True)\n",
    "            df.at[i, 'dropped'] = True\n",
    "            continue\n",
    "        tmp_embed.append(img_embed)\n",
    "        df.at[i, 'dropped'] = False\n",
    "        \n",
    "    df = df[df[\"dropped\"] != True]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return tmp_embed, df\n",
    "\n",
    "\n",
    "def df_tfrecords(df, output_fname):\n",
    "    import tensorflow as tf\n",
    "    from tfr_image.utils import bytes_feature, int64_feature\n",
    "\n",
    "    def image_to_tfexample(sample_id, image_data, image_format, height, width, caption):\n",
    "        return tf.train.Example(\n",
    "            features=tf.train.Features(\n",
    "                feature={\n",
    "                    \"sampleID\": bytes_feature(sample_id),\n",
    "                    \"image\": bytes_feature(image_data),\n",
    "                    \"format\": bytes_feature(image_format),\n",
    "                    \"label\": bytes_feature(caption),\n",
    "                    \"height\": int64_feature(height),\n",
    "                    \"width\": int64_feature(width),\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "    with tf.io.TFRecordWriter(output_fname) as tfrecord_writer:\n",
    "        for i in range(len(df)):\n",
    "            df_image = df.iloc[i]\n",
    "            image_fname = df_image[\"PATH\"]\n",
    "            file_type = image_fname.split(\".\")[-1]\n",
    "            with tf.io.gfile.GFile(image_fname, \"rb\") as f:\n",
    "                image_data = f.read()\n",
    "            example = image_to_tfexample(\n",
    "                str(df_image[\"SAMPLE_ID\"]).encode(\"utf_8\"),\n",
    "                image_data,\n",
    "                file_type.encode(\"utf_8\"),\n",
    "                df_image[\"HEIGHT\"],\n",
    "                df_image[\"WIDTH\"],\n",
    "                df_image[\"TEXT\"].encode(\"utf_8\"),\n",
    "            )\n",
    "            tfrecord_writer.write(example.SerializeToString())\n",
    "\n",
    "\n",
    "def filter(df, out_fname, output_folder):\n",
    "    results = []\n",
    "    #start0 = start = time.time()\n",
    "    img_embeddings, dff = df_clipfilter(df)\n",
    "    dff.to_csv(f\"{output_folder}{out_fname}.csv\", index=False, sep=\"|\")\n",
    "\n",
    "    #count results for each worker from resulting dff\n",
    "    dff[\"shard\"] = dff.apply(lambda row: str(row.PATH).split(\"/\")[1], axis=1)\n",
    "    results = dff[\"shard\"].value_counts()\n",
    "    #print(f\"CLIP ran in {round(time.time()-start,2)}\")\n",
    "    #start = time.time()\n",
    "    img_embeds_sampleid = {}\n",
    "    for i, img_embed_it in enumerate(img_embeddings):\n",
    "        dfid_index = dff.at[i, \"SAMPLE_ID\"]\n",
    "        img_embeds_sampleid[str(dfid_index)] = img_embed_it\n",
    "    with open(f\"{output_folder}image_embedding_dict-{out_fname}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(img_embeds_sampleid, f)\n",
    "    #print(f\"Embeddings ran in {round(time.time()-start,2)}\")\n",
    "    #start = time.time()\n",
    "    df_tfrecords(\n",
    "        dff,\n",
    "        f\"{output_folder}crawling_at_home_{out_fname}__00000-of-00001.tfrecord\",\n",
    "    )\n",
    "    #print(f\"Tfrecords ran in {round(time.time()-start,2)}\")\n",
    "    #print(f\"Job ran in {round(time.time()-start0,2)}\")\n",
    "    return len(dff), results\n"
   ]
  },
  {
   "source": [
    "# define workers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gpu_cah_interface(i:int, incomingqueue: JoinableQueue, outgoingqueue: JoinableQueue, YOUR_NICKNAME_FOR_THE_LEADERBOARD, CRAWLINGATHOME_SERVER_URL):\n",
    "    # initiate and reinitiate a GPU type client if needed\n",
    "    print (f\"   |___ inbound worker started\")\n",
    "    while True:\n",
    "        client = cah.init(\n",
    "            url=CRAWLINGATHOME_SERVER_URL, nickname=YOUR_NICKNAME_FOR_THE_LEADERBOARD, type=\"GPU\"\n",
    "        )\n",
    "        while client.isAlive():\n",
    "            while client.jobCount() > 0: \n",
    "                # each thread gets a new job, passes it to GPU then waits for completion\n",
    "                try:\n",
    "                    client.newJob()\n",
    "                except:\n",
    "                    time.sleep(10)\n",
    "                    continue\n",
    "                job = client.shard\n",
    "                os.mkdir(\"./\"+ job)\n",
    "                response = os.system(f\"rsync -rzh archiveteam@88.198.2.17::gpujobs/{job}/* {job}\") # no not delete just yet the source files\n",
    "                if response != 0:\n",
    "                    client.invalidURL()\n",
    "                    print (f\"[io] invalid job detected: {job}\")\n",
    "                    continue\n",
    "                else:\n",
    "                    os.system(f\"mv {job}/*_parsed.csv stats/\")\n",
    "                    os.system(f\"mv {job}/*_unfiltered.csv stats/\")\n",
    "                    print (f\"[io] job sent to GPU: {job}\")\n",
    "                    incomingqueue.put((i, job, client.upload_address))\n",
    "                \n",
    "                # wait until job gets processes\n",
    "                while True:\n",
    "                    if outgoingqueue.qsize() > 0:\n",
    "                        outjob, pairs = outgoingqueue.get() # I am poping out from queue only if my current job is finished\n",
    "                        print (f\"[io] received results for: {job}={outjob}\")\n",
    "                        outgoingqueue.task_done()\n",
    "                        if pairs > 0:\n",
    "                            print (f\"[io] mark job as complete: {job}\")\n",
    "                            client.completeJob(int(pairs))\n",
    "                        shutil.rmtree(\"./\"+ job)\n",
    "                        break # we can let the worker request a new job\n",
    "                    else:\n",
    "                        time.sleep(1)\n",
    "            else:\n",
    "                print (f\"[io] no jobs\")\n",
    "                time.sleep(10)\n",
    "        else:\n",
    "            print (f\"[io] client forgotten\")\n",
    "            time.sleep(10)\n",
    "\n",
    "def io_worker(incomingqueue: JoinableQueue, outgoingqueue: list, groupsize: int, YOUR_NICKNAME_FOR_THE_LEADERBOARD, CRAWLINGATHOME_SERVER_URL):\n",
    "    # separate process to initialize threaded workers\n",
    "    print (f\"[io] inbound workers:\")\n",
    "    try:\n",
    "        # just launch how many threads we need to group jobs into single output\n",
    "        for i in range(2 * groupsize):\n",
    "            threading.Thread(target=gpu_cah_interface, args=(i, incomingqueue, outgoingqueue[i], YOUR_NICKNAME_FOR_THE_LEADERBOARD, CRAWLINGATHOME_SERVER_URL)).start()\n",
    "    except Exception as e:\n",
    "        print(f\"[io] some inbound problem occured: {e}\")\n",
    "\n",
    "\n",
    "def gpu_worker(incomingqueue: JoinableQueue, outgoingqueue: list, counter: JoinableQueue, gpuflag: JoinableQueue, groupsize: int):\n",
    "    print (f\"[gpu] worker started\")\n",
    "    # watch for the incoming queue, when it is big enough we can trigger processing    \n",
    "    while True:\n",
    "        print (f\"[gpu] testing incoming queue size\")\n",
    "        if incomingqueue.qsize() >= groupsize:\n",
    "            gpuflag.put(1)\n",
    "            shards = []\n",
    "            addresses = []\n",
    "            group_id = uuid.uuid4().hex\n",
    "            print (f\"[gpu] got new {groupsize} jobs to group in id {group_id}\")\n",
    "            group_parse = None\n",
    "            for _ in range(groupsize):\n",
    "                i, job, address = incomingqueue.get()\n",
    "\n",
    "                all_csv_files = []\n",
    "                for path, subdir, files in os.walk(job):\n",
    "                    for file in glob(os.path.join(path, \"*.csv\")):\n",
    "                        all_csv_files.append(file)\n",
    "                # get name of csv file\n",
    "                out_path = all_csv_files[0]\n",
    "                shards.append((i, job, Path(out_path).stem.strip(\"_unfiltered\").strip(\"_parsed\").strip(\".\")))\n",
    "                addresses.append(address)\n",
    "\n",
    "                incomingqueue.task_done()\n",
    "            print (f\"[gpu] adjusted image paths\")\n",
    "\n",
    "            for i, job, item in shards:\n",
    "                dlparse_df = pd.read_csv(job + \"/\" + item + \".csv\", sep=\"|\")\n",
    "                dlparse_df[\"PATH\"] = dlparse_df.apply(lambda x: \"./\" + job + \"/\" + x[\"PATH\"].strip(\"save/\"), axis=1)\n",
    "                if group_parse is None:\n",
    "                    group_parse = dlparse_df\n",
    "                else:\n",
    "                    group_parse = group_parse.append(dlparse_df, ignore_index=True)\n",
    "                \n",
    "            with open(\"./save/\" + group_id + \".txt\", \"wt\") as f:\n",
    "                for i, job, item in shards:\n",
    "                    f.write(item + \"\\n\")\n",
    "            \n",
    "            print (f\"[gpu] saving stats\")\n",
    "\n",
    "            group_parse.to_csv(\"./stats/\" + group_id + \"_groupduped.csv\", index=False, sep=\"|\") # I am using these to find out domains to filter from scraping\n",
    "            group_parse.drop_duplicates(subset=[\"URL\",\"TEXT\"], keep='last', inplace=True)\n",
    "            group_parse.reset_index(inplace=True, drop=True)\n",
    "\n",
    "            group_parse.to_csv(\"./stats/\" + group_id + \"_groupdeduped.csv\", index=False, sep=\"|\") # I am using these to find out domains to filter from scraping\n",
    "\n",
    "            print (f\"[gpu] sending group to CLIP filter\")\n",
    "            start = time.time()\n",
    "            final_images, results = filter(group_parse, group_id, \"./save/\")\n",
    "            print(f\"last filtered {final_images} images in {round(time.time()-start,2)} sec\")\n",
    "\n",
    "            print (f\"[gpu] upload group results to rsync target\")\n",
    "            # find most required upload address among the grouped shards\n",
    "            upload_address = mode(addresses)\n",
    "            print (f\"most requested upload address is {upload_address}\")\n",
    "            response = os.system(f\"rsync -zh --remove-source-files save/*{group_id}* {upload_address}\") # to do get target from client\n",
    "            if response == 0:\n",
    "                print (f\"[gpu] sending all jobs to be marked as completed\")\n",
    "                for i, job, item in shards:\n",
    "                    outgoingqueue[i].put((job, results.get(job)))\n",
    "                    counter.put(1)\n",
    "            else:\n",
    "                for i, job, item in shards:\n",
    "                    outgoingqueue[i].put((job, 0)) # if upload crashes, then do NOT mark completeJob()\n",
    "            print (f\"[gpu] cleaning up group folders\")\n",
    "            \n",
    "            gpuflag.get()\n",
    "            gpuflag.task_done()\n",
    "        else:\n",
    "            time.sleep(10)"
   ]
  },
  {
   "source": [
    "# start all processes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io = Process(target=io_worker, args=[inbound, outbound, groupsize, YOUR_NICKNAME_FOR_THE_LEADERBOARD, CRAWLINGATHOME_SERVER_URL], daemon=True).start()\n",
    "\n",
    "gpu_worker(inbound, outbound, counter, gpuflag, groupsize)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('gpu': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "interpreter": {
   "hash": "1ba613d68b3b18985e02519c8e1689c7243aaf59a45b2d39d56345edb6b92440"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}