{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "! git clone \"https://github.com/TheoCoombes/crawlingathome\" crawlingathome_client\n",
    "! pip3 install -r crawlingathome_client/requirements.txt --no-cache-dir\n",
    "! rm requirements.txt\n",
    "! wget https://raw.githubusercontent.com/rvencu/crawlingathome-gpu-hcloud/staged-clients/requirements.txt\n",
    "! pip3 install -r ./requirements.txt --no-cache-dir\n",
    "! pip3 install ftfy pandas tfr_image\n",
    "! pip3 install tensorflow --no-cache-dir\n",
    "! pip3 install clip-anytorch\n",
    "! yes | pip3 uninstall pillow\n",
    "! CC=\"cc -mavx2\" pip3 install -U --force-reinstall pillow-simd"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into 'crawlingathome_client'...\n",
      "remote: Enumerating objects: 440, done.\u001b[K\n",
      "remote: Counting objects: 100% (127/127), done.\u001b[K\n",
      "remote: Compressing objects: 100% (107/107), done.\u001b[K\n",
      "remote: Total 440 (delta 76), reused 32 (delta 20), pack-reused 313\u001b[K\n",
      "Receiving objects: 100% (440/440), 116.07 KiB | 2.00 MiB/s, done.\n",
      "Resolving deltas: 100% (248/248), done.\n",
      "Requirement already satisfied: requests in /home/rvencu/anaconda3/envs/gpuhcloud/lib/python3.8/site-packages (from -r crawlingathome_client/requirements.txt (line 1)) (2.25.1)\n",
      "Requirement already satisfied: numpy in /home/rvencu/anaconda3/envs/gpuhcloud/lib/python3.8/site-packages (from -r crawlingathome_client/requirements.txt (line 2)) (1.21.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/rvencu/anaconda3/envs/gpuhcloud/lib/python3.8/site-packages (from requests->-r crawlingathome_client/requirements.txt (line 1)) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/rvencu/anaconda3/envs/gpuhcloud/lib/python3.8/site-packages (from requests->-r crawlingathome_client/requirements.txt (line 1)) (1.26.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/rvencu/anaconda3/envs/gpuhcloud/lib/python3.8/site-packages (from requests->-r crawlingathome_client/requirements.txt (line 1)) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/rvencu/anaconda3/envs/gpuhcloud/lib/python3.8/site-packages (from requests->-r crawlingathome_client/requirements.txt (line 1)) (2.10)\n",
      "rm: cannot remove 'requirements.txt': No such file or directory\n",
      "--2021-08-09 17:30:21--  https://raw.githubusercontent.com/rvencu/crawlingathome-gpu-hcloud/staged-clients/requirements.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2021-08-09 17:30:21 ERROR 404: Not Found.\n",
      "\n",
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: './requirements.txt'\u001b[0m\n",
      "Requirement already satisfied: ftfy in /home/rvencu/anaconda3/envs/gpuhcloud/lib/python3.8/site-packages (6.0.3)\n",
      "Requirement already satisfied: pandas in /home/rvencu/.local/lib/python3.8/site-packages (1.2.4)\n",
      "Requirement already satisfied: tfr_image in /home/rvencu/.local/lib/python3.8/site-packages (1.2)\n",
      "Requirement already satisfied: wcwidth in /home/rvencu/anaconda3/envs/gpuhcloud/lib/python3.8/site-packages (from ftfy) (0.2.5)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /home/rvencu/anaconda3/envs/gpuhcloud/lib/python3.8/site-packages (from pandas) (1.21.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/rvencu/.local/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/rvencu/anaconda3/envs/gpuhcloud/lib/python3.8/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/rvencu/.local/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: tensorflow in /home/rvencu/.local/lib/python3.8/site-packages (2.5.0)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in /home/rvencu/.local/lib/python3.8/site-packages (from tensorflow) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: six~=1.15.0 in /home/rvencu/.local/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /home/rvencu/.local/lib/python3.8/site-packages (from tensorflow) (3.1.0)\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /home/rvencu/.local/lib/python3.8/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: absl-py~=0.10 in /home/rvencu/.local/lib/python3.8/site-packages (from tensorflow) (0.13.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /home/rvencu/.local/lib/python3.8/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: wheel~=0.35 in /home/rvencu/.local/lib/python3.8/site-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in /home/rvencu/.local/lib/python3.8/site-packages (from tensorflow) (1.34.1)\n",
      "Requirement already satisfied: gast==0.4.0 in /home/rvencu/.local/lib/python3.8/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /home/rvencu/.local/lib/python3.8/site-packages (from tensorflow) (2.5.0)\n",
      "Collecting numpy~=1.19.2\n",
      "  Downloading numpy-1.19.5-cp38-cp38-manylinux2010_x86_64.whl (14.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.9 MB 3.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: termcolor~=1.1.0 in /home/rvencu/.local/lib/python3.8/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /home/rvencu/.local/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /home/rvencu/.local/lib/python3.8/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /home/rvencu/.local/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/rvencu/.local/lib/python3.8/site-packages (from tensorflow) (3.17.3)\n",
      "Requirement already satisfied: tensorboard~=2.5 in /home/rvencu/.local/lib/python3.8/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /home/rvencu/.local/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/rvencu/anaconda3/envs/gpuhcloud/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow) (52.0.0.post20210125)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/rvencu/.local/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/rvencu/.local/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/rvencu/.local/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow) (2.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/rvencu/.local/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow) (1.31.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/rvencu/.local/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/rvencu/anaconda3/envs/gpuhcloud/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/rvencu/.local/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow) (0.4.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/rvencu/anaconda3/envs/gpuhcloud/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/rvencu/.local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/rvencu/.local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/rvencu/.local/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/rvencu/anaconda3/envs/gpuhcloud/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/rvencu/anaconda3/envs/gpuhcloud/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.26.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/rvencu/anaconda3/envs/gpuhcloud/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/rvencu/anaconda3/envs/gpuhcloud/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/rvencu/anaconda3/envs/gpuhcloud/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/rvencu/anaconda3/envs/gpuhcloud/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (3.1.1)\n",
      "Installing collected packages: numpy, typing-extensions\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.1\n",
      "    Uninstalling numpy-1.21.1:\n",
      "      Successfully uninstalled numpy-1.21.1\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.0\n",
      "    Uninstalling typing-extensions-3.10.0.0:\n",
      "      Successfully uninstalled typing-extensions-3.10.0.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.10.0+cu111 requires pillow>=5.3.0, which is not installed.\n",
      "cudf 21.6.1+2.g101fc0fda4 requires cupy-cuda110, which is not installed.\n",
      "cudf 21.6.1+2.g101fc0fda4 requires Cython<0.30,>=0.29, which is not installed.\n",
      "datasets 1.8.0 requires tqdm<4.50.0,>=4.27, but you have tqdm 4.62.0 which is incompatible.\u001b[0m\n",
      "Successfully installed numpy-1.19.5 typing-extensions-3.10.0.0\n",
      "Requirement already satisfied: clip-anytorch in /home/rvencu/anaconda3/envs/gpuhcloud/lib/python3.8/site-packages (2.2.0)\n",
      "Requirement already satisfied: ftfy in /home/rvencu/anaconda3/envs/gpuhcloud/lib/python3.8/site-packages (from clip-anytorch) (6.0.3)\n",
      "Requirement already satisfied: regex in /home/rvencu/anaconda3/envs/gpuhcloud/lib/python3.8/site-packages (from clip-anytorch) (2021.8.3)\n",
      "Requirement already satisfied: torchvision in /home/rvencu/anaconda3/envs/gpuhcloud/lib/python3.8/site-packages (from clip-anytorch) (0.10.0+cu111)\n",
      "Requirement already satisfied: tqdm in /home/rvencu/anaconda3/envs/gpuhcloud/lib/python3.8/site-packages (from clip-anytorch) (4.62.0)\n",
      "Requirement already satisfied: torch in /home/rvencu/anaconda3/envs/gpuhcloud/lib/python3.8/site-packages (from clip-anytorch) (1.9.0+cu111)\n",
      "Requirement already satisfied: wcwidth in /home/rvencu/anaconda3/envs/gpuhcloud/lib/python3.8/site-packages (from ftfy->clip-anytorch) (0.2.5)\n",
      "Requirement already satisfied: typing-extensions in /home/rvencu/anaconda3/envs/gpuhcloud/lib/python3.8/site-packages (from torch->clip-anytorch) (3.10.0.0)\n",
      "Requirement already satisfied: numpy in /home/rvencu/anaconda3/envs/gpuhcloud/lib/python3.8/site-packages (from torchvision->clip-anytorch) (1.19.5)\n",
      "Collecting pillow>=5.3.0\n",
      "  Using cached Pillow-8.3.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0 MB)\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/home/rvencu/anaconda3/envs/gpuhcloud/lib/python3.8/site-packages/typing_extensions-3.10.0.0.dist-info/METADATA'\n",
      "\u001b[0m\n",
      "\u001b[33mWARNING: Skipping pillow as it is not installed.\u001b[0m\n",
      "yes: standard output: Broken pipe\n",
      "Collecting pillow-simd\n",
      "  Using cached Pillow_SIMD-7.0.0.post3-cp38-cp38-linux_x86_64.whl\n",
      "\u001b[33mWARNING: Error parsing requirements for typing-extensions: [Errno 2] No such file or directory: '/home/rvencu/anaconda3/envs/gpuhcloud/lib/python3.8/site-packages/typing_extensions-3.10.0.0.dist-info/METADATA'\u001b[0m\n",
      "Installing collected packages: pillow-simd\n",
      "  Attempting uninstall: pillow-simd\n",
      "    Found existing installation: Pillow-SIMD 7.0.0.post3\n",
      "    Uninstalling Pillow-SIMD-7.0.0.post3:\n",
      "      Successfully uninstalled Pillow-SIMD-7.0.0.post3\n",
      "Successfully installed pillow-simd-7.0.0.post3\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#@title Crawling at Home - GPU worker\n",
    "YOUR_NICKNAME_FOR_THE_LEADERBOARD = \"Colab-GPU-hcloud\" #@param {type:\"string\"}\n",
    "groupsize = 40 #@param {type:\"string\"}\n",
    "CRAWLINGATHOME_SERVER_URL = \"http://cah.io.community/\"\n",
    "groupsize = int(groupsize)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "import clip\n",
    "import shutil\n",
    "import torch\n",
    "import threading\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from colorama import Fore\n",
    "from statistics import mode\n",
    "import crawlingathome_client as cah\n",
    "from bloom_filter2 import BloomFilter\n",
    "sys.path.append('./crawlingathome-worker/')\n",
    "from multiprocessing import JoinableQueue, Process\n",
    "\n",
    "if not os.path.exists(\"./stats/\"):\n",
    "    os.makedirs(\"./stats/\")\n",
    "if not os.path.exists(\"./save/\"):\n",
    "    os.makedirs(\"./save/\")\n",
    "\n",
    "# initial cleanup - delete all working files in case of crash recovery\n",
    "reg_compile = re.compile(r\"^\\d{1,3}-\\d{1,3}-\\d{1,3}-\\d{1,3}$\")\n",
    "for root, dirnames, filenames in os.walk(\".\"):\n",
    "    for filename in filenames:\n",
    "        if filename.startswith(\"gpujob.zip_\"):\n",
    "            os.remove(filename)\n",
    "    for dir in dirnames:\n",
    "        if reg_compile.match(dir):\n",
    "            shutil.rmtree(dir)\n",
    "re_uuid = re.compile(r'[0-9a-f]{32}', re.I)\n",
    "for root, dirnames, filenames in os.walk(\".\"):\n",
    "    for dir in dirnames:\n",
    "        if re_uuid.match(dir):\n",
    "            shutil.rmtree(dir)\n",
    "re_gz = re.compile(r'.*.tar.gz.*', re.I)\n",
    "for root, dirnames, filenames in os.walk(\".\"):\n",
    "    for file in filenames:\n",
    "        if re_gz.match(file):\n",
    "            os.remove(file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "#initialize joinable queues to transfer messages between multiprocess processes\n",
    "# Outbound queues, we need one for each io worker\n",
    "outbound = []\n",
    "for _ in range(int(2.7 * groupsize)): # we need 2x IO workers to keep GPU permanently busy\n",
    "        outbound.append(JoinableQueue())\n",
    "inbound = JoinableQueue()\n",
    "uploadqueue = JoinableQueue()\n",
    "counter = JoinableQueue()\n",
    "inpsize = JoinableQueue() # use this to communicate number of jobs downloading now\n",
    "gpuflag = JoinableQueue() # use this to flag that gpu is processing"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# define CLIP class around OpenAI clip model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "class CLIPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, preprocess):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_transform = preprocess\n",
    "        self.tokenizer = clip.tokenize\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.dataframe.iloc[index]\n",
    "        return (\n",
    "            self.image_transform(Image.open(row[\"PATH\"])),\n",
    "            self.tokenizer(row[\"TEXT\"], truncate=True)[0],\n",
    "        )\n",
    "\n",
    "class CLIP:\n",
    "    def __init__(self):\n",
    "        self.model, self.preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "        self.cosine_similarity = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        with torch.no_grad():\n",
    "            self.categories = self.model.encode_text(clip.tokenize([\"neutral\",\"selfie\", \"illustration, drawing\", \"toys, play, kids, children\", \"teddy bear, puppet\", \"animal, bird, mammal, insect\" \"fashion, clothes\", \"logo, commercial, ad, advertisement\", \"drawing, painting\",\"anime, cartoon\",\"comedy, fun\",\"romance, love story\",\"thriller, suspense, crime story\",\"action, action movie\", \"horror, monster movie\", \"documentary\", \"news, journalism\", \"entertainment\", \"talk show\", \"porn, sex, sperm, nipples, breats, tits, boops, penis, dick, cock, clitoris, vagina, fuck, lust, horny, sexual, lick, licking\",  \"porn, sex, sperm, nipples\", \"porn, sex, sperm, penis, dick, cock\", \"nipples, breats, tits, boops, sexy\", \"penis, dick, cock\", \"clitoris, vagina\", \"sex, fuck, lust, horny, sexual, lick, licking\", \"porn, sex, sexy\",\"sexy, hot\",\"sperm, skin\",\"lust, horny, sexual\",\"lick, licking, body\", \"anime, hentai, sexy\", \"cartoon, sexy, sex\", \"hentai\", \"anime, sexy, breasts\", \"hentai\"]).to(device))\n",
    "            self.underaged_categories = self.model.encode_text(clip.tokenize([\"teenager, teen\", \"kid, child, teenager, teen, baby or toddler, underaged, little girl, little boy\", \"kid, child, little girl, little boy\", \"baby, toddler\",\"adult, woman, man, grownup, grown person,full-aged of legal age\",\"full-aged, of legal age, adult\",\"woman, man\",\"adult, woman, man, grownup, grown person,full-aged of legal age\"]).to(device))\n",
    "            self.animal_categories = self.model.encode_text(clip.tokenize([\"lifeless object, thing\", \"thing, object\", \"material\", \"furniture\",\"wall\", \"house\", \"tree\", \"wood\",\"ground\",\"industry\", \"table\", \"bed\", \"tool\", \"dress, clothes\", \"door\", \"chair\", \"rock, stone\", \"human\", \"man\", \"woman\", \"man, woman\", \"animal\",\"cat\",\"dog\", \"cow\", \"pig\", \"goat\", \"sheep\", \"elephant\", \"horse\", \"horse, elephant, pig, dog, cat, sheep, goat, animal\", \"life\", \"wildlife\"]).to(device))\n",
    "\n",
    "    def similarity_imgalt(self, image_tensor, text_tokens):\n",
    "        with torch.no_grad():\n",
    "            image_features = self.model.encode_image(image_tensor.to(device)).float()\n",
    "            text_features = self.model.encode_text(text_tokens.to(device)).float()\n",
    "            similarity = self.cosine_similarity(image_features, text_features).tolist()\n",
    "\n",
    "        image_features = image_features.detach().cpu().numpy()\n",
    "        return image_features, similarity\n",
    "\n",
    "    def preprocess_images(self, df):\n",
    "        ret_image_features = []\n",
    "        ret_similarity = []\n",
    "        batch_size = 256 if device == \"cuda\" else 8\n",
    "        dataset = CLIPDataset(df, self.preprocess)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=int(2*cpu_count()/3), pin_memory=True)\n",
    "        for tensors, tokens in dataloader:\n",
    "            image_features, similarities = self.similarity_imgalt(tensors, tokens)\n",
    "            ret_image_features.extend(image_features)\n",
    "            ret_similarity.extend(similarities)\n",
    "        return ret_image_features, ret_similarity\n",
    "\n",
    "    def prob(self, image_features, text_features):\n",
    "        text_features = text_features.float()\n",
    "        image_features = torch.as_tensor(image_features).to(device, dtype=torch.float32)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "        _, indices = similarity.topk(2)\n",
    "        return indices\n",
    "\n",
    "\n",
    "clip_filter = CLIP()\n",
    "\n",
    "\n",
    "def df_clipfilter(df):\n",
    "    sim_threshold = 0.3\n",
    "    underaged_text = [\"teen\", \"kid\", \"child\", \"baby\"]\n",
    "\n",
    "    img_embedding, similarities = clip_filter.preprocess_images(df)\n",
    "    tmp_embed = []\n",
    "\n",
    "    for i, img_embed in enumerate(img_embedding):\n",
    "        if similarities[i] < sim_threshold:\n",
    "            #df.drop(i, inplace=True)\n",
    "            df.at[i, 'dropped'] = True\n",
    "            continue\n",
    "\n",
    "        # get most similar categories\n",
    "        nsfw_prob = clip_filter.prob(img_embed, clip_filter.categories)\n",
    "        df.at[i, \"NSFW\"] = \"UNSURE\"\n",
    "        df.at[i, \"similarity\"] = similarities[i]\n",
    "        if nsfw_prob[0] < 19 and nsfw_prob[1] < 19:\n",
    "            df.at[i, \"NSFW\"] = \"UNLIKELY\"\n",
    "            tmp_embed.append(img_embed)\n",
    "            continue\n",
    "        elif nsfw_prob[0] >= 19 and nsfw_prob[1] >= 19:\n",
    "            df.at[i, \"NSFW\"] = \"NSFW\"\n",
    "\n",
    "        underage_prob = clip_filter.prob(img_embed, clip_filter.underaged_categories)\n",
    "        if underage_prob[0] < 4 or underage_prob[1] < 4 or any(x in df.at[i, \"TEXT\"] for x in underaged_text):\n",
    "            #df.drop(i, inplace=True)\n",
    "            df.at[i, 'dropped'] = True\n",
    "            continue\n",
    "\n",
    "        animal_prob = clip_filter.prob(img_embed, clip_filter.animal_categories)\n",
    "        if animal_prob[0] > 20:\n",
    "            #df.drop(i, inplace=True)\n",
    "            df.at[i, 'dropped'] = True\n",
    "            continue\n",
    "        tmp_embed.append(img_embed)\n",
    "        df.at[i, 'dropped'] = False\n",
    "        \n",
    "    df = df[df[\"dropped\"] != True]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return tmp_embed, df\n",
    "\n",
    "\n",
    "def df_tfrecords(df, output_fname):\n",
    "    import tensorflow as tf\n",
    "    from tfr_image.utils import bytes_feature, int64_feature\n",
    "\n",
    "    def image_to_tfexample(sample_id, image_data, image_format, height, width, caption):\n",
    "        return tf.train.Example(\n",
    "            features=tf.train.Features(\n",
    "                feature={\n",
    "                    \"sampleID\": bytes_feature(sample_id),\n",
    "                    \"image\": bytes_feature(image_data),\n",
    "                    \"format\": bytes_feature(image_format),\n",
    "                    \"label\": bytes_feature(caption),\n",
    "                    \"height\": int64_feature(height),\n",
    "                    \"width\": int64_feature(width),\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "    with tf.io.TFRecordWriter(output_fname) as tfrecord_writer:\n",
    "        for i in range(len(df)):\n",
    "            df_image = df.iloc[i]\n",
    "            image_fname = df_image[\"PATH\"]\n",
    "            file_type = image_fname.split(\".\")[-1]\n",
    "            with tf.io.gfile.GFile(image_fname, \"rb\") as f:\n",
    "                image_data = f.read()\n",
    "            example = image_to_tfexample(\n",
    "                str(df_image[\"SAMPLE_ID\"]).encode(\"utf_8\"),\n",
    "                image_data,\n",
    "                file_type.encode(\"utf_8\"),\n",
    "                df_image[\"HEIGHT\"],\n",
    "                df_image[\"WIDTH\"],\n",
    "                df_image[\"TEXT\"].encode(\"utf_8\"),\n",
    "            )\n",
    "            tfrecord_writer.write(example.SerializeToString())\n",
    "\n",
    "\n",
    "def filter(df, out_fname, output_folder):\n",
    "    results = []\n",
    "    #start0 = start = time.time()\n",
    "    img_embeddings, dff = df_clipfilter(df)\n",
    "    dff.to_csv(f\"{output_folder}{out_fname}.csv\", index=False, sep=\"|\")\n",
    "\n",
    "    #count results for each worker from resulting dff\n",
    "    dff[\"shard\"] = dff.apply(lambda row: str(row.PATH).split(\"/\")[1], axis=1)\n",
    "    results = dff[\"shard\"].value_counts()\n",
    "    #print(f\"CLIP ran in {round(time.time()-start,2)}\")\n",
    "    #start = time.time()\n",
    "    img_embeds_sampleid = {}\n",
    "    for i, img_embed_it in enumerate(img_embeddings):\n",
    "        dfid_index = dff.at[i, \"SAMPLE_ID\"]\n",
    "        img_embeds_sampleid[str(dfid_index)] = img_embed_it\n",
    "    with open(f\"{output_folder}image_embedding_dict-{out_fname}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(img_embeds_sampleid, f)\n",
    "    #print(f\"Embeddings ran in {round(time.time()-start,2)}\")\n",
    "    #start = time.time()\n",
    "    df_tfrecords(\n",
    "        dff,\n",
    "        f\"{output_folder}crawling_at_home_{out_fname}__00000-of-00001.tfrecord\",\n",
    "    )\n",
    "    #print(f\"Tfrecords ran in {round(time.time()-start,2)}\")\n",
    "    #print(f\"Job ran in {round(time.time()-start0,2)}\")\n",
    "    return len(dff), results\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# define workers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def gpu_cah_interface(i:int, incomingqueue: JoinableQueue, outgoingqueue: JoinableQueue, YOUR_NICKNAME_FOR_THE_LEADERBOARD, CRAWLINGATHOME_SERVER_URL):\n",
    "    # initiate and reinitiate a GPU type client if needed\n",
    "    print (f\"   |___ inbound worker started\")\n",
    "    while True:\n",
    "        client = cah.init(\n",
    "            url=CRAWLINGATHOME_SERVER_URL, nickname=YOUR_NICKNAME_FOR_THE_LEADERBOARD, type=\"GPU\"\n",
    "        )\n",
    "        while client.isAlive():\n",
    "            while client.jobCount() > 0: \n",
    "                # each thread gets a new job, passes it to GPU then waits for completion\n",
    "                try:\n",
    "                    client.newJob()\n",
    "                except:\n",
    "                    time.sleep(10)\n",
    "                    continue\n",
    "                job = \"\"\n",
    "                try:\n",
    "                    job = client.shard.split(\" \")[1]\n",
    "                except:\n",
    "                    client.invalidURL()\n",
    "                    print (f\"[io {i}] invalid job detected: {job}\")\n",
    "                    continue\n",
    "                # found repeating shards, need to clear old files before continuing\n",
    "                if os.path.exists(\"./\"+ job):\n",
    "                    shutil.rmtree(\"./\"+ job, ignore_errors=True)\n",
    "                os.mkdir(\"./\"+ job)\n",
    "                client.downloadShard()\n",
    "\n",
    "                if len(glob(f\"{job}/*.csv\")) == 0:\n",
    "                    client.invalidURL()\n",
    "                    print (f\"[io {i}] invalid job detected: {job}\")\n",
    "                    continue\n",
    "                for file in glob(f\"{job}/*_parsed.csv\"):\n",
    "                    os.system(f\"mv {file} stats/\")\n",
    "                for file in glob(f\"{job}/*_unfiltered.csv\"):\n",
    "                    os.system(f\"mv {file} stats/\")\n",
    "                #print (f\"[io] job sent to GPU: {job}\")\n",
    "                incomingqueue.put((i, job, client.upload_address))\n",
    "                \n",
    "                # wait until job gets processes\n",
    "                while True:\n",
    "                    if outgoingqueue.qsize() > 0:\n",
    "                        outjob, pairs = outgoingqueue.get() # I am poping out from queue only if my current job is finished\n",
    "                        print (f\"[io] received results for: {job}={outjob}\")\n",
    "                        outgoingqueue.task_done()\n",
    "                        if pairs > 0:\n",
    "                            print (f\"[io] mark job as complete: {job}\")\n",
    "                            try:\n",
    "                                client.completeJob(int(pairs))\n",
    "                            except:\n",
    "                                pass\n",
    "                        shutil.rmtree(\"./\"+ job)\n",
    "                        break # we can let the worker request a new job\n",
    "                    else:\n",
    "                        time.sleep(1)\n",
    "            else:\n",
    "                print (f\"[io] no jobs\")\n",
    "                time.sleep(10)\n",
    "        else:\n",
    "            print (f\"[io] client forgotten\")\n",
    "            time.sleep(10)\n",
    "\n",
    "def io_worker(incomingqueue: JoinableQueue, outgoingqueue: list, groupsize: int, YOUR_NICKNAME_FOR_THE_LEADERBOARD, CRAWLINGATHOME_SERVER_URL):\n",
    "    # separate process to initialize threaded workers\n",
    "    print (f\"[io] inbound workers:\")\n",
    "    try:\n",
    "        # just launch how many threads we need to group jobs into single output\n",
    "        for i in range(2 * groupsize):\n",
    "            threading.Thread(target=gpu_cah_interface, args=(i, incomingqueue, outgoingqueue[i], YOUR_NICKNAME_FOR_THE_LEADERBOARD, CRAWLINGATHOME_SERVER_URL)).start()\n",
    "    except Exception as e:\n",
    "        print(f\"[io] some inbound problem occured: {e}\")\n",
    "\n",
    "\n",
    "def gpu_worker(incomingqueue: JoinableQueue, outgoingqueue: list, counter: JoinableQueue, gpuflag: JoinableQueue, groupsize: int):\n",
    "    print (f\"[gpu] worker started\")\n",
    "    # watch for the incoming queue, when it is big enough we can trigger processing    \n",
    "    while True:\n",
    "        print (f\"[gpu] testing incoming queue size\")\n",
    "        if incomingqueue.qsize() >= groupsize:\n",
    "            gpuflag.put(1)\n",
    "            shards = []\n",
    "            addresses = []\n",
    "            group_id = uuid.uuid4().hex\n",
    "            print (f\"[gpu] got new {groupsize} jobs to group in id {group_id}\")\n",
    "            group_parse = None\n",
    "            for _ in range(groupsize):\n",
    "                i, job, address = incomingqueue.get()\n",
    "\n",
    "                all_csv_files = []\n",
    "                for path, subdir, files in os.walk(job):\n",
    "                    for file in glob(os.path.join(path, \"*.csv\")):\n",
    "                        all_csv_files.append(file)\n",
    "                # get name of csv file\n",
    "                out_path = all_csv_files[0]\n",
    "                shards.append((i, job, Path(out_path).stem.strip(\"_unfiltered\").strip(\"_parsed\").strip(\".\")))\n",
    "                addresses.append(address)\n",
    "\n",
    "                incomingqueue.task_done()\n",
    "            print (f\"[gpu] adjusted image paths\")\n",
    "\n",
    "            for i, job, item in shards:\n",
    "                dlparse_df = pd.read_csv(job + \"/\" + item + \".csv\", sep=\"|\")\n",
    "                dlparse_df[\"PATH\"] = dlparse_df.apply(lambda x: \"./\" + job + \"/\" + x[\"PATH\"].strip(\"save/\"), axis=1)\n",
    "                if group_parse is None:\n",
    "                    group_parse = dlparse_df\n",
    "                else:\n",
    "                    group_parse = group_parse.append(dlparse_df, ignore_index=True)\n",
    "                \n",
    "            with open(\"./save/\" + group_id + \".txt\", \"wt\") as f:\n",
    "                for i, job, item in shards:\n",
    "                    f.write(item + \"\\n\")\n",
    "            \n",
    "            print (f\"[gpu] saving stats\")\n",
    "\n",
    "            group_parse.to_csv(\"./stats/\" + group_id + \"_groupduped.csv\", index=False, sep=\"|\") # I am using these to find out domains to filter from scraping\n",
    "            group_parse.drop_duplicates(subset=[\"URL\",\"TEXT\"], keep='last', inplace=True)\n",
    "            group_parse.reset_index(inplace=True, drop=True)\n",
    "\n",
    "            group_parse.to_csv(\"./stats/\" + group_id + \"_groupdeduped.csv\", index=False, sep=\"|\") # I am using these to find out domains to filter from scraping\n",
    "\n",
    "            print (f\"[gpu] sending group to CLIP filter\")\n",
    "            start = time.time()\n",
    "            final_images, results = filter(group_parse, group_id, \"./save/\")\n",
    "            \n",
    "            total = len(group_parse.index)\n",
    "            dedupe_ratio = round((duped - total) / duped, 2)\n",
    "            print(f\"filtered {final_images} from {total} deduped from {duped} (dedupe ratio {dedupe_ratio}) in {round(time.time()-start,2)} sec ({groupsize})\")\n",
    "\n",
    "            print (f\"[gpu] upload group results to rsync target\")\n",
    "            # find most required upload address among the grouped shards\n",
    "            upload_address = mode(addresses)\n",
    "            print (f\"most requested upload address is {upload_address}\")\n",
    "            response = os.system(f\"rsync -zh --remove-source-files save/*{group_id}* {upload_address}\") # to do get target from client\n",
    "            if response == 0:\n",
    "                print (f\"[gpu] sending all jobs to be marked as completed\")\n",
    "                for i, job, item in shards:\n",
    "                    outgoingqueue[i].put((job, results.get(job)))\n",
    "                    counter.put(1)\n",
    "            else:\n",
    "                for i, job, item in shards:\n",
    "                    outgoingqueue[i].put((job, 0)) # if upload crashes, then do NOT mark completeJob()\n",
    "            print (f\"[gpu] cleaning up group folders\")\n",
    "\n",
    "            if final_images < 7500:\n",
    "                groupsize += 2\n",
    "                print (f\"groupsize changed to {groupsize}\")\n",
    "            if final_images > 8500:\n",
    "                groupsize -= 2\n",
    "                print (f\"groupsize changed to {groupsize}\")\n",
    "            \n",
    "            gpuflag.get()\n",
    "            gpuflag.task_done()\n",
    "        else:\n",
    "            time.sleep(10)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# start all processes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "io = Process(target=io_worker, args=[inbound, outbound, groupsize, YOUR_NICKNAME_FOR_THE_LEADERBOARD, CRAWLINGATHOME_SERVER_URL], daemon=True).start()\n",
    "\n",
    "gpu_worker(inbound, outbound, counter, gpuflag, groupsize)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[io] inbound workers:\n",
      "   |___ inbound worker started   |___ inbound worker started   |___ inbound worker started\n",
      "   |___ inbound worker started   |___ inbound worker started\n",
      "\n",
      "   |___ inbound worker started\n",
      "   |___ inbound worker started[17:33 crawling@home] connecting to crawling@home server...\n",
      "[17:33 crawling@home] connecting to crawling@home server...[17:33 crawling@home] connecting to crawling@home server...\n",
      "\n",
      "[17:33 crawling@home] connecting to crawling@home server...   |___ inbound worker started\n",
      "\n",
      "   |___ inbound worker started\n",
      "[17:33 crawling@home] connecting to crawling@home server...\n",
      "\n",
      "\n",
      "[17:33 crawling@home] connecting to crawling@home server...[17:33 crawling@home] connecting to crawling@home server...[17:33 crawling@home] connecting to crawling@home server...\n",
      "   |___ inbound worker started\n",
      "[17:33 crawling@home] connecting to crawling@home server...\n",
      "   |___ inbound worker started   |___ inbound worker started   |___ inbound worker started   |___ inbound worker started\n",
      "   |___ inbound worker started   |___ inbound worker started\n",
      "   |___ inbound worker started\n",
      "\n",
      "\n",
      "   |___ inbound worker started\n",
      "\n",
      "\n",
      "[17:33 crawling@home] connecting to crawling@home server...\n",
      "   |___ inbound worker started   |___ inbound worker started\n",
      "[17:33 crawling@home] connecting to crawling@home server...[17:33 crawling@home] connecting to crawling@home server...   |___ inbound worker started[17:33 crawling@home] connecting to crawling@home server...\n",
      "[17:33 crawling@home] connecting to crawling@home server...\n",
      "\n",
      "[17:33 crawling@home] connecting to crawling@home server...\n",
      "\n",
      "   |___ inbound worker started[17:33 crawling@home] connecting to crawling@home server...\n",
      "\n",
      "[17:33 crawling@home] connecting to crawling@home server...   |___ inbound worker started\n",
      "[17:33 crawling@home] connecting to crawling@home server...\n",
      "[17:33 crawling@home] connecting to crawling@home server...\n",
      "\n",
      "\n",
      "   |___ inbound worker started[17:33 crawling@home] connecting to crawling@home server...\n",
      "   |___ inbound worker started\n",
      "\n",
      "   |___ inbound worker started\n",
      "[17:33 crawling@home] connecting to crawling@home server...[17:33 crawling@home] connecting to crawling@home server...   |___ inbound worker started[17:33 crawling@home] connecting to crawling@home server...\n",
      "   |___ inbound worker started\n",
      "\n",
      "\n",
      "   |___ inbound worker started   |___ inbound worker started\n",
      "\n",
      "\n",
      "   |___ inbound worker started\n",
      "[17:33 crawling@home] connecting to crawling@home server...[17:33 crawling@home] connecting to crawling@home server...[17:33 crawling@home] connecting to crawling@home server...\n",
      "\n",
      "\n",
      "[17:33 crawling@home] connecting to crawling@home server...\n",
      "   |___ inbound worker started\n",
      "[17:33 crawling@home] connecting to crawling@home server...\n",
      "   |___ inbound worker started[17:33 crawling@home] connecting to crawling@home server...   |___ inbound worker started\n",
      "\n",
      "\n",
      "\n",
      "   |___ inbound worker started   |___ inbound worker started\n",
      "   |___ inbound worker started\n",
      "[17:33 crawling@home] connecting to crawling@home server...[17:33 crawling@home] connecting to crawling@home server...   |___ inbound worker started[17:33 crawling@home] connecting to crawling@home server...[17:33 crawling@home] connecting to crawling@home server...\n",
      "\n",
      "\n",
      "[17:33 crawling@home] connecting to crawling@home server...\n",
      "\n",
      "   |___ inbound worker started[17:33 crawling@home] connecting to crawling@home server...   |___ inbound worker started\n",
      "   |___ inbound worker started\n",
      "\n",
      "[17:33 crawling@home] connecting to crawling@home server...   |___ inbound worker started\n",
      "[17:33 crawling@home] connecting to crawling@home server...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "   |___ inbound worker started   |___ inbound worker started   |___ inbound worker started\n",
      "   |___ inbound worker started\n",
      "[17:33 crawling@home] connecting to crawling@home server...\n",
      "\n",
      "[17:33 crawling@home] connecting to crawling@home server...   |___ inbound worker started\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[17:33 crawling@home] connecting to crawling@home server...[17:33 crawling@home] connecting to crawling@home server...   |___ inbound worker started[17:33 crawling@home] connecting to crawling@home server...[17:33 crawling@home] connecting to crawling@home server...\n",
      "[17:33 crawling@home] connecting to crawling@home server...   |___ inbound worker started\n",
      "\n",
      "\n",
      "[17:33 crawling@home] connecting to crawling@home server...\n",
      "[17:33 crawling@home] connecting to crawling@home server...\n",
      "   |___ inbound worker started   |___ inbound worker started\n",
      "\n",
      "[17:33 crawling@home] connecting to crawling@home server...\n",
      "\n",
      "[17:33 crawling@home] connecting to crawling@home server...   |___ inbound worker started\n",
      "\n",
      "\n",
      "   |___ inbound worker started   |___ inbound worker started\n",
      "   |___ inbound worker started\n",
      "\n",
      "   |___ inbound worker started[17:33 crawling@home] connecting to crawling@home server...[17:33 crawling@home] connecting to crawling@home server...\n",
      "   |___ inbound worker started\n",
      "[17:33 crawling@home] connecting to crawling@home server...[17:33 crawling@home] connecting to crawling@home server...\n",
      "\n",
      "   |___ inbound worker started\n",
      "\n",
      "[17:33 crawling@home] connecting to crawling@home server..."
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('gpuhcloud': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "bc322c11e8113b1b1dfcd753c5702c5c5d95a81c495f9a7060b170a2a7888bca"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}